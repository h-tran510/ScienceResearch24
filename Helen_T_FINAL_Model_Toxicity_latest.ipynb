{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuRZMfGACHin"
      },
      "source": [
        "# Imports/Downloads\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckSAow8gGw7V",
        "outputId": "aea46697-8e4a-4434-8bd2-9a58d9fe43ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.9.5-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.1.0)\n",
            "Downloading rdkit-2024.9.5-cp311-cp311-manylinux_2_28_x86_64.whl (34.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.9.5\n",
            "Collecting selfies\n",
            "  Downloading selfies-2.2.0-py3-none-any.whl.metadata (14 kB)\n",
            "Downloading selfies-2.2.0-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: selfies\n",
            "Successfully installed selfies-2.2.0\n",
            "Collecting datamol\n",
            "  Downloading datamol-0.12.5-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from datamol) (4.67.1)\n",
            "Collecting loguru (from datamol)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from datamol) (1.4.2)\n",
            "Requirement already satisfied: fsspec>=2021.9 in /usr/local/lib/python3.11/dist-packages (from datamol) (2024.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datamol) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from datamol) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from datamol) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from datamol) (3.10.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from datamol) (11.1.0)\n",
            "Requirement already satisfied: selfies in /usr/local/lib/python3.11/dist-packages (from datamol) (2.2.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from datamol) (4.3.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from datamol) (1.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datamol) (24.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from datamol) (4.12.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from datamol) (6.5.2)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (from datamol) (2024.9.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->datamol) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->datamol) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->datamol) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->datamol) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->datamol) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->datamol) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datamol) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datamol) (2025.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->datamol) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->datamol) (1.17.0)\n",
            "Downloading datamol-0.12.5-py3-none-any.whl (495 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.4/495.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru, datamol\n",
            "Successfully installed datamol-0.12.5 loguru-0.7.3\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.13)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "Collecting torch_scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch_scatter\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (2024.9.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit\n",
        "!pip install selfies\n",
        "!pip install datamol\n",
        "!pip install scikit-learn\n",
        "!pip install torch-geometric\n",
        "!pip install torch_scatter\n",
        "!pip install torch\n",
        "!pip install rdkit\n",
        "!pip install pandas\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_scatter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_BI0jqsx5HL",
        "outputId": "787e63da-479b-43f8-efa0-d40d9b81d27b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_scatter\n",
            "  Using cached torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch_scatter\n",
            "  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=3629840 sha256=8424fbd4ecf1aa8b59778fc1caaa7a31f0fec4e2d44892faff959076e3d31013\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\n",
            "Successfully built torch_scatter\n",
            "Installing collected packages: torch_scatter\n",
            "Successfully installed torch_scatter-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI1Kt1tsGPV7"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spCWqZn3GODJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (ConfusionMatrixDisplay, confusion_matrix,\n",
        "                             roc_auc_score, precision_score, recall_score,\n",
        "                             f1_score, roc_curve)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from rdkit import Chem, RDLogger\n",
        "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
        "from rdkit.Chem import Draw\n",
        "import datamol as dm\n",
        "import selfies as sf\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Bilinear, Linear, Parameter, Sequential\n",
        "\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import global_add_pool\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.inits import glorot, reset\n",
        "\n",
        "from torch_scatter import scatter\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "dm.disable_rdkit_log()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bf6RENdAmKr"
      },
      "outputs": [],
      "source": [
        "# Sets seed for reproducibility\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXqkuiwffIbC"
      },
      "source": [
        "# Molecular Standardization + Defining Input\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yltz4G2gNpBJ"
      },
      "outputs": [],
      "source": [
        "smiles_column = \"smiles\"\n",
        "\n",
        "def preprocess(row):\n",
        "    \"\"\"\n",
        "\n",
        "    Preprocesses smiles strings/mols for molecular standardization\n",
        "\n",
        "    \"\"\"\n",
        "    # Iterates over rows to\n",
        "    if row is None:\n",
        "      return None\n",
        "\n",
        "    mol = dm.to_mol(row[smiles_column], ordered=True)\n",
        "    if mol is None:\n",
        "      return None\n",
        "\n",
        "    mol = dm.fix_mol(mol)\n",
        "    mol = dm.sanitize_mol(mol, sanifix=True, charge_neutral=False)\n",
        "    mol = dm.standardize_mol(\n",
        "        mol,\n",
        "        disconnect_metals = False,\n",
        "        normalize = True,\n",
        "        reionize = True,\n",
        "        uncharge = False,\n",
        "        stereo = True,\n",
        "    )\n",
        "\n",
        "    row[\"standard_smiles\"] = dm.standardize_smiles(dm.to_smiles(mol))\n",
        "    row[\"selfies\"] = dm.to_selfies(mol)\n",
        "    row[\"inchi\"] = dm.to_inchi(mol)\n",
        "    row[\"inchikey\"] = dm.to_inchikey(mol)\n",
        "\n",
        "    return row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF-5wyNVCCZP"
      },
      "source": [
        "# Defining Functions for Graph Feature Extraction\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr_9JTHBj5c-"
      },
      "outputs": [],
      "source": [
        "def one_hot_encoding(x, permitted_list):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Maps input elements x which are not in the permitted list to the last element\n",
        "    of the permitted list.\n",
        "\n",
        "    \"\"\"\n",
        "    if x not in permitted_list:\n",
        "        x = permitted_list[-1]\n",
        "    binary_encoding = [int(boolean_value) for boolean_value in list(map(lambda s: x == s, permitted_list))]\n",
        "    return binary_encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfKNyP_2hxAq"
      },
      "outputs": [],
      "source": [
        "def process_atoms(mol):\n",
        "  \"\"\"\n",
        "\n",
        "  Makes sure there are no none atoms\n",
        "\n",
        "  \"\"\"\n",
        "  atom_features_list = []\n",
        "\n",
        "  for atom in mol.GetAtoms():\n",
        "    atom_features = get_atom_features(atom)\n",
        "\n",
        "    if atom is None:\n",
        "      atom_features_list.append(atom_features)\n",
        "      print(f\"Skipping invalid atom with index: {atom.GetIdx()}\")\n",
        "      # append with zero vector with len of the feature vector\n",
        "      atom_features_list.append(np.zeros(len(atom_features)))\n",
        "    else:\n",
        "       atom_features_list.append(atom_features)\n",
        "\n",
        "  return atom_features_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHtaBRZSDXjs"
      },
      "outputs": [],
      "source": [
        "def get_atom_features(atom,\n",
        "                      use_chirality=True,\n",
        "                      hydrogens_implicit=True):\n",
        "    \"\"\"\n",
        "\n",
        "    Takes an RDKit atom object as input and gives a 1d-numpy array of atom features as output.\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if atom is None:\n",
        "            # Handle the case where the atom is None\n",
        "            return np.zeros(0)  # Returning a zero vector as a fallback\n",
        "\n",
        "        # Proceed with feature extraction\n",
        "        permitted_list_of_atoms = ['C','N','O','S','F','Si','P','Cl','Br','Mg','Na','Ca','Fe','As','Al','I', 'B','V','K','Tl','Yb','Sb','Sn','Ag','Pd','Co','Se','Ti','Zn', 'Li','Ge','Cu','Au','Ni','Cd','In','Mn','Zr','Cr','Pt','Hg','Pb','Unknown']\n",
        "\n",
        "        if hydrogens_implicit == False:\n",
        "            permitted_list_of_atoms = ['H'] + permitted_list_of_atoms\n",
        "\n",
        "        atom_type_enc = one_hot_encoding(str(atom.GetSymbol()), permitted_list_of_atoms)\n",
        "        n_heavy_neighbors_enc = one_hot_encoding(int(atom.GetDegree()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
        "        formal_charge_enc = one_hot_encoding(int(atom.GetFormalCharge()), [-3, -2, -1, 0, 1, 2, 3, \"Extreme\"])\n",
        "        hybridisation_type_enc = one_hot_encoding(str(atom.GetHybridization()), [\"S\", \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\", \"OTHER\"])\n",
        "        is_in_a_ring_enc = [int(atom.IsInRing())]\n",
        "        is_aromatic_enc = [int(atom.GetIsAromatic())]\n",
        "        atomic_mass_scaled = [float((atom.GetMass() - 10.812) / 116.092)]\n",
        "        vdw_radius_scaled = [float((Chem.GetPeriodicTable().GetRvdw(atom.GetAtomicNum()) - 1.5) / 0.6)]\n",
        "        covalent_radius_scaled = [float((Chem.GetPeriodicTable().GetRcovalent(atom.GetAtomicNum()) - 0.64) / 0.76)]\n",
        "\n",
        "        atom_feature_vector = atom_type_enc + n_heavy_neighbors_enc + formal_charge_enc + hybridisation_type_enc + is_in_a_ring_enc + is_aromatic_enc + atomic_mass_scaled + vdw_radius_scaled + covalent_radius_scaled\n",
        "\n",
        "        if use_chirality:\n",
        "            chirality_type_enc = one_hot_encoding(str(atom.GetChiralTag()), [\"CHI_UNSPECIFIED\", \"CHI_TETRAHEDRAL_CW\", \"CHI_TETRAHEDRAL_CCW\", \"CHI_OTHER\"])\n",
        "            atom_feature_vector += chirality_type_enc\n",
        "\n",
        "        if hydrogens_implicit:\n",
        "            n_hydrogens_enc = one_hot_encoding(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
        "            atom_feature_vector += n_hydrogens_enc\n",
        "\n",
        "        return np.array(atom_feature_vector)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in processing atom {atom}: {e}\")\n",
        "        # Return a zero vector if any exception is encountered\n",
        "        return np.zeros(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q90jjGsUlEfu"
      },
      "outputs": [],
      "source": [
        "def get_bond_features(bond,\n",
        "                      use_stereochemistry = True):\n",
        "    \"\"\"\n",
        "\n",
        "    Takes an RDKit bond object as input and gives a 1d-numpy array of bond features as output.\n",
        "\n",
        "    \"\"\"\n",
        "    if bond is None:\n",
        "      return np.zeros(4)\n",
        "\n",
        "    permitted_list_of_bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE, Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
        "    bond_type_enc = one_hot_encoding(bond.GetBondType(), permitted_list_of_bond_types)\n",
        "\n",
        "    bond_is_conj_enc = [int(bond.GetIsConjugated())]\n",
        "\n",
        "    bond_is_in_ring_enc = [int(bond.IsInRing())]\n",
        "\n",
        "    bond_feature_vector = bond_type_enc + bond_is_conj_enc + bond_is_in_ring_enc\n",
        "\n",
        "    if use_stereochemistry == True:\n",
        "        stereo_type_enc = one_hot_encoding(str(bond.GetStereo()), [\"STEREOZ\", \"STEREOE\", \"STEREOANY\", \"STEREONONE\"])\n",
        "        bond_feature_vector += stereo_type_enc\n",
        "\n",
        "    return np.array(bond_feature_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5wLBB7tlsVn"
      },
      "source": [
        "# Create Graph Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ln0EB0knlpk4"
      },
      "outputs": [],
      "source": [
        "def smiles_to_data(smiles, label):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "\n",
        "    x_smiles = [smiles_1, smiles_2, ....] ... a list of SMILES strings\n",
        "    y = [y_1, y_2, ...] ... a list of numerial labels for the SMILES strings (such as associated pKi values)\n",
        "\n",
        "    Outputs:\n",
        "\n",
        "    data_list = [G_1, G_2, ...] ... a list of torch_geometric.data.Data objects which represent labeled molecular graphs that can readily be used for machine learning\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    data_list = []\n",
        "\n",
        "    # convert SMILES to RDKit mol object\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    # if mol is None:\n",
        "    #   print(f\"Invalid SMILES string: {smiles}\")\n",
        "    #   return None\n",
        "\n",
        "    #processing atoms to make sure there are no NoneType atoms\n",
        "    atom_features_list = process_atoms(mol)\n",
        "\n",
        "    # get feature dimensions\n",
        "    n_nodes = len(atom_features_list)\n",
        "    n_edges = 2 * mol.GetNumBonds()\n",
        "    unrelated_smiles = \"O=O\"\n",
        "    unrelated_mol = Chem.MolFromSmiles(unrelated_smiles)\n",
        "\n",
        "    n_node_features = len(get_atom_features(unrelated_mol.GetAtomWithIdx(0)))\n",
        "    n_edge_features = len(get_bond_features(unrelated_mol.GetBondBetweenAtoms(0, 1)))\n",
        "\n",
        "    # construct node feature matrix X_features of shape (n_nodes, n_node_features)\n",
        "    X_features = np.zeros((n_nodes, n_node_features))\n",
        "\n",
        "    for atom in mol.GetAtoms():\n",
        "      atom_features = get_atom_features(atom)\n",
        "      X_features[atom.GetIdx(), :] = get_atom_features(atom)\n",
        "\n",
        "    X_features = torch.tensor(X_features, dtype = torch.float)\n",
        "\n",
        "    # construct edge index array\n",
        "    (rows, cols) = np.nonzero(GetAdjacencyMatrix(mol))\n",
        "\n",
        "    torch_rows = torch.from_numpy(rows.astype(np.int64)).to(torch.long)\n",
        "    torch_cols = torch.from_numpy(cols.astype(np.int64)).to(torch.long)\n",
        "    edge_index = torch.stack([torch_rows, torch_cols], dim = 0)\n",
        "\n",
        "    # construct edge feature array of shape (n_edges, n_edge_features)\n",
        "    edge_attr = np.zeros((n_edges, n_edge_features))\n",
        "\n",
        "    for (k, (i,j)) in enumerate(zip(rows, cols)):\n",
        "        edge_attr[k] = get_bond_features(mol.GetBondBetweenAtoms(int(i),int(j)))\n",
        "\n",
        "    edge_attr = torch.tensor(edge_attr, dtype = torch.float)\n",
        "\n",
        "    # construct label tensor\n",
        "    y_tensor = torch.tensor(np.array([label]), dtype = torch.long)\n",
        "\n",
        "    # construct Pytorch Geometric data object and append to data list\n",
        "    data_list.append(Data(x = X_features, edge_index = edge_index, edge_attr = edge_attr, y = y_tensor))\n",
        "\n",
        "    return data_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5kQmZ8ypKPe"
      },
      "source": [
        "# Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wChjyRRpo6Tr"
      },
      "outputs": [],
      "source": [
        "class FeatureAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Feature attention\n",
        "\n",
        "    Reference:\n",
        "    https://github.com/idrugLab/hignn/blob/main/source/model.py\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, channels, reduction):\n",
        "        super().__init__()\n",
        "        self.mlp = Sequential(\n",
        "            Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            Linear(channels // reduction, channels, bias=False),\n",
        "        )\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        reset(self.mlp)\n",
        "\n",
        "    def forward(self, x, batch, size=None):\n",
        "        max_result = scatter(x, batch, dim=0, dim_size=size, reduce=\"max\")\n",
        "        sum_result = scatter(x, batch, dim=0, dim_size=size, reduce=\"sum\")\n",
        "        max_out = self.mlp(max_result)\n",
        "        sum_out = self.mlp(sum_result)\n",
        "        y = torch.sigmoid(max_out + sum_out)\n",
        "        y = y[batch]\n",
        "        return x * y\n",
        "\n",
        "class NTNConv(MessagePassing):\n",
        "   \"\"\"\n",
        "    Neural Tensor Network Convolutional Layer\n",
        "\n",
        "    Reference:\n",
        "    https://github.com/idrugLab/hignn/blob/main/source/model.py\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, in_channels, out_channels, slices, dropout, edge_dim=None, **kwargs\n",
        "    ):\n",
        "        kwargs.setdefault(\"aggr\", \"add\")\n",
        "        super(NTNConv, self).__init__(node_dim=0, **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.slices = slices\n",
        "        self.dropout = dropout\n",
        "        self.edge_dim = edge_dim\n",
        "\n",
        "        self.weight_node = Parameter(torch.Tensor(in_channels, out_channels))\n",
        "        if edge_dim is not None:\n",
        "            self.weight_edge = Parameter(torch.Tensor(edge_dim, out_channels))\n",
        "        else:\n",
        "            self.weight_edge = self.register_parameter(\"weight_edge\", None)\n",
        "\n",
        "        self.bilinear = Bilinear(out_channels, out_channels, slices, bias=False)\n",
        "\n",
        "        if self.edge_dim is not None:\n",
        "            self.linear = Linear(3 * out_channels, slices)\n",
        "        else:\n",
        "            self.linear = Linear(2 * out_channels, slices)\n",
        "\n",
        "        self._alpha = None\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot(self.weight_node)\n",
        "        glorot(self.weight_edge)\n",
        "        self.bilinear.reset_parameters()\n",
        "        self.linear.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr=None, return_attention_weights=None):\n",
        "        x = torch.matmul(x, self.weight_node)\n",
        "\n",
        "        if self.weight_edge is not None:\n",
        "            assert edge_attr is not None\n",
        "            edge_attr = torch.matmul(edge_attr, self.weight_edge)\n",
        "\n",
        "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
        "\n",
        "        alpha = self._alpha\n",
        "        self._alpha = None\n",
        "\n",
        "        if isinstance(return_attention_weights, bool):\n",
        "            assert alpha is not None\n",
        "            return out, (edge_index, alpha)\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "    def message(self, x_i, x_j, edge_attr):\n",
        "        score = self.bilinear(x_i, x_j)\n",
        "        if edge_attr is not None:\n",
        "            vec = torch.cat((x_i, edge_attr, x_j), 1)\n",
        "            block_score = self.linear(vec)  # bias already included\n",
        "        else:\n",
        "            vec = torch.cat((x_i, x_j), 1)\n",
        "            block_score = self.linear(vec)\n",
        "        scores = score + block_score\n",
        "        alpha = torch.tanh(scores)\n",
        "        self._alpha = alpha\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "\n",
        "        dim_split = self.out_channels // self.slices\n",
        "        out = torch.max(x_j, edge_attr).view(-1, self.slices, dim_split)\n",
        "\n",
        "        out = out * alpha.view(-1, self.slices, 1)\n",
        "        out = out.view(-1, self.out_channels)\n",
        "        return out\n",
        "\n",
        "\n",
        "class CustomGNN(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        hidden_channels,\n",
        "        out_channels,\n",
        "        edge_dim,\n",
        "        num_layers,\n",
        "        slices,\n",
        "        dropout,\n",
        "        f_att = True,\n",
        "        r=4,\n",
        "    ):\n",
        "        super(CustomGNN, self).__init__()\n",
        "\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.f_att = f_att\n",
        "\n",
        "        # atom feature transformation\n",
        "        self.lin_a = Linear(in_channels, hidden_channels)\n",
        "        self.lin_b = Linear(edge_dim, hidden_channels)\n",
        "\n",
        "        # convs block\n",
        "        self.atom_convs = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            conv = NTNConv(\n",
        "                hidden_channels,\n",
        "                hidden_channels,\n",
        "                slices=slices,\n",
        "                dropout=dropout,\n",
        "                edge_dim=hidden_channels,\n",
        "            )\n",
        "            self.atom_convs.append(conv)\n",
        "\n",
        "        self.lin_gate = Linear(3 * hidden_channels, hidden_channels)\n",
        "\n",
        "        if self.f_att:\n",
        "            self.feature_att = FeatureAttention(channels=hidden_channels, reduction=r)\n",
        "\n",
        "        self.out = Linear(hidden_channels, out_channels)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin_a.reset_parameters()\n",
        "        self.lin_b.reset_parameters()\n",
        "\n",
        "        for conv in self.atom_convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "        self.lin_gate.reset_parameters()\n",
        "\n",
        "        if self.f_att:\n",
        "            self.feature_att.reset_parameters()\n",
        "\n",
        "        self.out.reset_parameters()\n",
        "\n",
        "    def forward(self, data):\n",
        "        # get mol input\n",
        "        x = data.x\n",
        "        edge_index = data.edge_index\n",
        "        edge_attr = data.edge_attr\n",
        "        batch = data.batch\n",
        "\n",
        "        x = F.relu(self.lin_a(x))  # (N, 46) -> (N, hidden_channels)\n",
        "        edge_attr = F.relu(self.lin_b(edge_attr))  # (N, 10) -> (N, hidden_channels)\n",
        "\n",
        "        # mol conv block\n",
        "        for i in range(0, self.num_layers):\n",
        "            h = F.relu(self.atom_convs[i](x, edge_index, edge_attr))\n",
        "            beta = self.lin_gate(torch.cat([x, h, x - h], 1)).sigmoid()\n",
        "            x = beta * x + (1 - beta) * h\n",
        "            if self.f_att:\n",
        "                x = self.feature_att(x, batch)\n",
        "\n",
        "        mol_vec = global_add_pool(x, batch).relu_()\n",
        "        out = F.dropout(mol_vec, p=self.dropout, training=self.training)\n",
        "        return self.out(out), out\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-class Focal Loss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        \"\"\"\n",
        "        input: [N, C], float32\n",
        "        target: [N, ], int64\n",
        "        \"\"\"\n",
        "        eps = 1e-10  # Avoid inf for log\n",
        "        p = F.softmax(x, dim=1)\n",
        "        logits = (1 - p) ** self.gamma * torch.log(p + eps)\n",
        "        loss = F.nll_loss(logits, target)\n",
        "        return loss\n",
        "\n",
        "class CenterLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Center loss\n",
        "\n",
        "    Reference:\n",
        "    https://github.com/KaiyangZhou/pytorch-center-loss/blob/master/center_loss.py\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10, feat_dim=2):\n",
        "        super(CenterLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.feat_dim = feat_dim\n",
        "        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim)) # (2, 128)\n",
        "\n",
        "    def forward(self, x, target): # (batch_size, last_feat_size) -> (32, 128)\n",
        "        subtraction = x - 2*torch.matmul(F.one_hot(target).float(), self.centers)\n",
        "        loss = torch.mean(torch.pow(subtraction, 2))  / 2\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNuuj2qN9uWJ"
      },
      "source": [
        "# Define Training, Testing, and Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SABN52etpymS"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, optimizer_cent, criterion_focal, criterion_cent, train_loader):\n",
        "    model.train()\n",
        "    device = torch.device(\"cpu\")\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        optimizer_cent.zero_grad()\n",
        "        out, feat_out = model(data)\n",
        "        loss = criterion_focal(out, data.y.view(-1)) + 0.01* criterion_cent(feat_out, data.y.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer_cent.step()\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    device = torch.device(\"cpu\")\n",
        "    correct = 0\n",
        "    y_pred, y_true = [], []\n",
        "\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        out, _ = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        y_pred.extend(pred.tolist())\n",
        "        y_true.extend(data.y.view(-1).tolist())\n",
        "        correct += (pred == data.y.view(-1)).sum().item()\n",
        "    auc = roc_auc_score(y_true, y_pred)\n",
        "    cf = confusion_matrix(y_true, y_pred)\n",
        "    acc = correct / len(test_loader.dataset)\n",
        "    precision = precision_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, pos_label=1)\n",
        "    f1 = f1_score(y_true, y_pred, pos_label=1)\n",
        "    return acc, auc, cf, precision, recall, f1\n",
        "\n",
        "\n",
        "def get_data_distribution(df):\n",
        "    best_dist = []\n",
        "    for task in list(df.columns.values)[1:]:\n",
        "        # print(f\"Processing {task}...\")\n",
        "        count = dict(df[task].value_counts(dropna=True))\n",
        "        # print(count)\n",
        "        zeros = count[0.0]\n",
        "        ones = count[1.0]\n",
        "\n",
        "        dist = ones / (ones + zeros)\n",
        "        # print(f\"Total: {ones+zeros}, ones: {ones}, ratio: {dist}\")\n",
        "        if dist > 0.5 and dist < 0.6 and len(best_dist) < 10:\n",
        "            best_dist.append((task, dist))\n",
        "    # print(f\"Best task: {_label}, distribution: {best_dist}\")\n",
        "    return best_dist\n",
        "\n",
        "\n",
        "def save_fig(cf, parent_dir, fig_name):\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cf, display_labels=[\"0\", \"1\"])\n",
        "    output_path = os.path.join(parent_dir, f\"{fig_name}.png\")\n",
        "    disp.plot()\n",
        "    disp.figure_.savefig(output_path)\n",
        "    plt.close(\"all\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amTzG1dEsPw_"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9ZZIdQoT0M6"
      },
      "outputs": [],
      "source": [
        " df = pd.read_csv(\"./toxcast_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rXwuIKYqS7K",
        "outputId": "803f8fe1-69ce-4186-ca5b-1470e127c2aa",
        "scrolled": true,
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in original dataset: 8597\n",
            "Number of tasks: 617\n",
            "Number of tasks that aren't imbalanced: 10\n",
            "Some Tasks: [('CLD_CYP1A2_48hr', 0.5148514851485149), ('CLD_CYP2B6_24hr', 0.5761589403973509), ('CLD_CYP2B6_48hr', 0.5412541254125413), ('NCCT_HEK293T_CellTiterGLO', 0.5386666666666666), ('NCCT_QuantiLum_inhib_dn', 0.5517241379310345), ('NCCT_TPO_GUA_dn', 0.5849056603773585), ('NVS_ADME_hCYP3A4', 0.5341614906832298), ('NVS_ADME_rCYP2C12', 0.5818181818181818), ('NVS_ENZ_hBACE', 0.5294117647058824), ('NVS_ENZ_hDUSP3', 0.5865384615384616)]\n",
            "===> Processing CLD_CYP1A2_48hr, 1/10...\n",
            "Number of rows after preprocessing: 300\n",
            "Number of training data: 240\n",
            "    - Number of ones: 122\n",
            "    - Number of zeros: 118\n",
            "Number of test data: 60\n",
            "    - Number of ones: 31\n",
            "    - Number of zeros: 29\n",
            "Epoch:  10, Loss: 0.6808\n",
            "    - Train Acc 0.5958, Train AUC: 0.5912, Train Precision: 0.5668, Train Recall: 0.8689, Train F1: 0.6861\n",
            "    - Test Acc: 0.6000, Test AUC: 0.5873, Test Precision: 0.5660, Test Recall: 0.9677, Test F1: 0.7143\n",
            "Epoch:  20, Loss: 0.6758\n",
            "    - Train Acc 0.6042, Train AUC: 0.5986, Train Precision: 0.5672, Train Recall: 0.9344, Train F1: 0.7059\n",
            "    - Test Acc: 0.5500, Test AUC: 0.5356, Test Precision: 0.5357, Test Recall: 0.9677, Test F1: 0.6897\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "if __name__ == \"__main__\":\n",
        "    df = pd.read_csv(\"./toxcast_data.csv\")\n",
        "\n",
        "    # Determines data with best data distribution\n",
        "    print(f\"Number of rows in original dataset: {df.shape[0]}\")\n",
        "    print(f\"Number of tasks: {df.shape[1] - 1}\")\n",
        "\n",
        "    best_dist = get_data_distribution(df)\n",
        "    print(f\"Number of tasks that aren't imbalanced: {len(best_dist)}\")\n",
        "    print(f\"Some Tasks: {best_dist[:10]}\")\n",
        "\n",
        "    df = df.apply(preprocess, axis = 1)\n",
        "\n",
        "    # Initiates the training loop for x feature and at x features done/y features total\n",
        "    for i, task in enumerate(best_dist):\n",
        "        print(f\"===> Processing {task[0]}, {i+1}/{len(best_dist)}...\")\n",
        "\n",
        "        # Determine in_channels before the loop\n",
        "        unrelated_smiles = \"O=O\"\n",
        "        unrelated_mol = Chem.MolFromSmiles(unrelated_smiles)\n",
        "        n_node_features = len(get_atom_features(unrelated_mol.GetAtomWithIdx(0)))\n",
        "        ### Load model\n",
        "        model = CustomGNN(\n",
        "            in_channels = n_node_features,\n",
        "            hidden_channels = 128,\n",
        "            out_channels = 2,\n",
        "            edge_dim = 10,\n",
        "            num_layers = 3,\n",
        "            dropout = 0.3,\n",
        "            slices =2 ,\n",
        "            f_att = True,\n",
        "            r = 4,\n",
        "        )\n",
        "\n",
        "        # Initialize criterion and optimizer\n",
        "        criterion_focal = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "        criterion_cent = CenterLoss(num_classes=2, feat_dim=128)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.0005, weight_decay=5e-4)\n",
        "        optimizer_cent = torch.optim.Adam(criterion_cent.parameters(), lr = 0.5, weight_decay=5e-4)\n",
        "\n",
        "        ### Load data\n",
        "        labels = []\n",
        "        data_list = []\n",
        "        for idx, row in df.iterrows():\n",
        "            if not (pd.isna(row[task[0]])):\n",
        "                data_obj_list = smiles_to_data(row[\"standard_smiles\"], row[task[0]])\n",
        "                if data_obj_list is not None:\n",
        "                    if len(data_obj_list) > 1:\n",
        "                        print(\"Consider error...\")\n",
        "                    for data_obj in data_obj_list:\n",
        "                        labels.append(data_obj.y)\n",
        "                        data_list.append(data_obj)\n",
        "\n",
        "        print(f\"Number of rows after preprocessing: {len(data_list)}\")\n",
        "\n",
        "        # Split into Training and Testing Sets\n",
        "        batch_size = 32\n",
        "        train_data, test_data = train_test_split(\n",
        "            data_list, test_size=0.2, random_state=42, stratify=labels\n",
        "        )\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        train_ones = [1 for sample in train_data if sample.y == 1]\n",
        "        train_zeros = [0 for sample in train_data if sample.y == 0]\n",
        "        test_ones = [1 for sample in test_data if sample.y == 1]\n",
        "        test_zeros = [0 for sample in test_data if sample.y == 0]\n",
        "        print(f\"Number of training data: {len(train_data)}\")\n",
        "        print(f\"    - Number of ones: {len(train_ones)}\")\n",
        "        print(f\"    - Number of zeros: {len(train_zeros)}\")\n",
        "        print(f\"Number of test data: {len(test_data)}\")\n",
        "        print(f\"    - Number of ones: {len(test_ones)}\")\n",
        "        print(f\"    - Number of zeros: {len(test_zeros)}\")\n",
        "\n",
        "        # Training loop\n",
        "        num_epochs = 100\n",
        "        best_auc, best_acc, best_precision, best_recall, best_f1 = [0] * 5\n",
        "        best_auc_epoch, best_acc_epoch, best_f1_epoch = [0] * 3\n",
        "\n",
        "        # Storing metrics for plotting\n",
        "        train_loss = []\n",
        "        train_acc_history = []\n",
        "        test_acc_history = []\n",
        "        train_loss_history = []\n",
        "        epochs = []\n",
        "\n",
        "        model_path = \"best_models\"\n",
        "        fig_path = \"figs\"\n",
        "        metrics_name = \"metrics.txt\"\n",
        "        folder = \"tasks-final-3\"\n",
        "        Path(os.path.join(folder, task[0], model_path)).mkdir(\n",
        "            parents=True, exist_ok=True\n",
        "        )\n",
        "        Path(os.path.join(folder, task[0], fig_path)).mkdir(\n",
        "            parents=True, exist_ok=True\n",
        "        )\n",
        "        output_file = open(os.path.join(folder, task[0], metrics_name), \"w\")\n",
        "\n",
        "        start = time.time()\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "            loss = train(model, optimizer, optimizer_cent, criterion_focal, criterion_cent, train_loader)\n",
        "            train_acc, train_auc, train_cf, train_precision, train_recall, train_f1 = test(model, train_loader)\n",
        "            test_acc, test_auc, test_cf, test_precision, test_recall, test_f1 = test(model, test_loader)\n",
        "\n",
        "            train_loss_history.append(loss)\n",
        "            epochs.append(epoch)\n",
        "\n",
        "            train_acc_history.append(train_acc)\n",
        "            test_acc_history.append(test_acc)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch: {epoch:3d}, Loss: {loss:.4f}\")\n",
        "                print(f\"    - Train Acc {train_acc:.4f}, Train AUC: {train_auc:.4f}, Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train F1: {train_f1:.4f}\")\n",
        "                print(f\"    - Test Acc: {test_acc:.4f}, Test AUC: {test_auc:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}\")\n",
        "\n",
        "                if epoch > 100:\n",
        "                    if test_auc > best_auc:\n",
        "                        best_auc_epoch = epoch\n",
        "                        best_auc = test_auc\n",
        "                        save_fig(\n",
        "                            train_cf, os.path.join(folder, task[0], fig_path), \"train\"\n",
        "                        )\n",
        "                        save_fig(test_cf, os.path.join(folder, task[0], fig_path), \"test\")\n",
        "                        torch.save(\n",
        "                            model.state_dict(),\n",
        "                            os.path.join(folder, task[0], model_path, \"best_auc_model.pt\"),\n",
        "                        )\n",
        "                    if test_acc > best_acc:\n",
        "                        best_acc_epoch = epoch\n",
        "                        best_acc = test_acc\n",
        "                        torch.save(\n",
        "                            model.state_dict(),\n",
        "                            os.path.join(folder, task[0], model_path, \"best_acc_model.pt\"),\n",
        "                        )\n",
        "                    if test_f1 > best_f1:\n",
        "                        best_f1_epoch = epoch\n",
        "                        best_f1 = test_f1\n",
        "                        best_precision = test_precision\n",
        "                        best_recall = test_recall\n",
        "                        torch.save(\n",
        "                            model.state_dict(),\n",
        "                            os.path.join(folder, task[0], model_path, \"best_f1_model.pt\"),\n",
        "                        )\n",
        "        end = time.time() - start\n",
        "        output_file.write(f\"Training time: {end} seconds\\n\")\n",
        "        output_file.write(f\"Epoch of best_acc on test set: {best_acc_epoch}\\n\")\n",
        "        output_file.write(f\"Best acc on test set: {best_acc}\\n\")\n",
        "        output_file.write(f\"Epoch of best_auc on test set: {best_auc_epoch}\\n\")\n",
        "        output_file.write(f\"Best AUC on test set: {best_auc}\\n\")\n",
        "        output_file.write(f\"Epoch of best_f1 on test set: {best_f1_epoch}\\n\")\n",
        "        output_file.write(f\"Best f1 on test set: {best_f1}\\n\")\n",
        "        output_file.write(f\"Best precision on test set: {best_precision}\\n\")\n",
        "        output_file.write(f\"Best recall on test set: {best_recall}\\n\")\n",
        "        output_file.close()\n",
        "\n",
        "        # Create an accuracy curve plot for the current feature\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(epochs, train_acc_history, label=\"Train Accuracy\")\n",
        "        plt.plot(epochs, test_acc_history, label=\"Test Accuracy\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.title(f\"Accuracy Curve for Feature: {task[0]}\")\n",
        "        plt.legend()\n",
        "        # Build a unique filename using the feature/task name\n",
        "        fig_dir = os.path.join(folder, task[0], fig_path)\n",
        "        fig_filename = os.path.join(fig_dir, f\"accuracy_curve_{task[0]}.png\")\n",
        "        plt.savefig(fig_filename)\n",
        "        plt.close()\n",
        "        print(f\"Saved accuracy curve for {task[0]} at {fig_filename}\")\n",
        "\n",
        "        # Optionally, create a loss curve plot as well\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(epochs, train_loss_history, label=\"Training Loss\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(f\"Training Loss Curve for Feature: {task[0]}\")\n",
        "        plt.legend()\n",
        "        loss_filename = os.path.join(fig_dir, f\"loss_curve_{task[0]}.png\")\n",
        "        plt.savefig(loss_filename)\n",
        "        plt.close()\n",
        "        print(f\"Saved loss curve for {task[0]} at {loss_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google.colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zc0vMwbJ2x8",
        "outputId": "7720d487-c2a4-4a8e-a7bd-de9ded80661f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google.colab in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: google-auth==2.27.0 in /usr/local/lib/python3.11/dist-packages (from google.colab) (2.27.0)\n",
            "Requirement already satisfied: ipykernel==6.17.1 in /usr/local/lib/python3.11/dist-packages (from google.colab) (6.17.1)\n",
            "Requirement already satisfied: ipyparallel==8.8.0 in /usr/local/lib/python3.11/dist-packages (from google.colab) (8.8.0)\n",
            "Requirement already satisfied: ipython==7.34.0 in /usr/local/lib/python3.11/dist-packages (from google.colab) (7.34.0)\n",
            "Requirement already satisfied: notebook==6.5.5 in /usr/local/lib/python3.11/dist-packages (from google.colab) (6.5.5)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (from google.colab) (2.2.2)\n",
            "Requirement already satisfied: portpicker==1.5.2 in /usr/local/lib/python3.11/dist-packages (from google.colab) (1.5.2)\n",
            "Requirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from google.colab) (2.32.3)\n",
            "Requirement already satisfied: tornado==6.4.2 in /usr/local/lib/python3.11/dist-packages (from google.colab) (6.4.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth==2.27.0->google.colab) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth==2.27.0->google.colab) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth==2.27.0->google.colab) (4.9)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google.colab) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google.colab) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google.colab) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google.colab) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google.colab) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google.colab) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google.colab) (24.0.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google.colab) (5.7.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipyparallel==8.8.0->google.colab) (4.4.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from ipyparallel==8.8.0->google.colab) (0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from ipyparallel==8.8.0->google.colab) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from ipyparallel==8.8.0->google.colab) (4.67.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google.colab) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython==7.34.0->google.colab)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google.colab) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google.colab) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google.colab) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google.colab) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google.colab) (4.9.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google.colab) (3.1.5)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google.colab) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google.colab) (5.7.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google.colab) (0.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google.colab) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google.colab) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google.colab) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google.colab) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google.colab) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.5->google.colab) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->google.colab) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->google.colab) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->google.colab) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->google.colab) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->google.colab) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->google.colab) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->google.colab) (2025.1.31)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython==7.34.0->google.colab) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.1->notebook==6.5.5->google.colab) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook==6.5.5->google.colab) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.5->google.colab) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook==6.5.5->google.colab) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.5->google.colab) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.5->google.colab) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.5->google.colab) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.5->google.colab) (3.1.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.5->google.colab) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.5->google.colab) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook==6.5.5->google.colab) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook==6.5.5->google.colab) (4.23.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython==7.34.0->google.colab) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.34.0->google.colab) (0.2.13)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth==2.27.0->google.colab) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->ipyparallel==8.8.0->google.colab) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook==6.5.5->google.colab) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook==6.5.5->google.colab) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook==6.5.5->google.colab) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google.colab) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google.colab) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google.colab) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.5->google.colab) (0.23.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google.colab) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook==6.5.5->google.colab) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook==6.5.5->google.colab) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook==6.5.5->google.colab) (4.12.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook==6.5.5->google.colab) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google.colab) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google.colab) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.5->google.colab) (1.3.1)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.move(\"/content/tasks-final-2\", \"/content/drive/MyDrive/Helen Tran - Science Research 2024-2025/Final Data\") # Use the imported shutil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ROJ-TeegHwQt",
        "outputId": "e8bde3d6-7a2b-47cb-a285-7bbe3e776df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Helen Tran - Science Research 2024-2025/Final Data/tasks-final-2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvIslzhcJbsT",
        "outputId": "e16f01d4-2a3f-460f-cc78-a8f916e6f448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}