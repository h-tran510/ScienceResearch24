# -*- coding: utf-8 -*-
"""Helen T - Model Toxicity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VfIW3p4kZk9TZNhmHCCMLuinzAKiPIBC

#Downloads and Imports

Installation for the following:


*   Pytorch Geometric (this might take like 44 minutes to download btw)
*   rdkit
*   selfies
*   datamol
"""

pip install rdkit

pip install selfies

pip install datamol

pip install scikit-learn

!pip install --verbose --no-cache-dir torch-scatter
!pip install --verbose --no-cache-dir torch-sparse
!pip install --verbose --no-cache-dir torch-cluster
!pip install torch-geometric
!pip install tensorboardX
!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip ngrok-stable-linux-amd64.zip

from google.colab import drive
drive.mount('/content/drive')

# Load/import libraries
import numpy as np
import pandas as pd
import tensorflow as tf

from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt

from rdkit import Chem
from rdkit.Chem import PandasTools
import datamol as dm
import selfies as sf

from tensorflow import keras

dm.disable_rdkit_log()

"""#Preprocessing"""

# loading the datasets in

# Idk if this will work on your end because I copied the file path from my Google Drive

# tox_21 = '/content/drive/MyDrive/Helen Tran - Science Research 2024-2025/Data/tox21_10k_data_all.sdf'

# tox_21 = PandasTools.LoadSDF(tox_21, embedProps = True, molColName = None, smilesName = 'smiles')
#clintox = pd.read_csv('/content/drive/MyDrive/Helen Tran - Science Research 2024-2025/Data/clintox.csv')
toxcast = pd.read_csv('/content/drive/MyDrive/Helen Tran - Science Research 2024-2025/Data/toxcast_data.csv')

#NaN = unknown data/no data
#0/1 = no interaction
#remove all rows/columns with NaNs

# tox_df['NR-AR'] just gives the values in a specific column, 'NR-AR' is specific to this dataset
# notna excludes all NaNs
# pd.merge (df1, df2) merges the dfs together
# pd.merge(df1,df2, on=‘Formula’) keeps the Formula column constant, but all the other columns are merged

"""## Removing Invalid Values"""

# removing nans from toxcast
smiles_data = toxcast['smiles']
feature_data = toxcast.drop(columns = ['smiles'])
feature_data_clean = feature_data.notna().astype(float)
smiles_clean = smiles_data.loc[feature_data_clean.index]
toxcast_clean = feature_data_clean.copy()
toxcast_clean['smiles'] = smiles_clean

toxcast_clean = toxcast_clean.merge(smiles_clean, on = 'smiles')
print(toxcast_clean)

# #removing nans from clintox
# smiles_clintox = clintox['smiles']
# clintox_features= clintox.drop(columns = ['smiles'])
# clintox_features_clean = clintox_features.notna().astype(float)
# smiles_clintox_clean = smiles_clintox.loc[clintox_features_clean.index]
# clintox_clean = clintox_features_clean.copy()
# clintox_clean['smiles'] = smiles_clintox_clean
# print(clintox_clean)

"""## Molecular Standardization"""

# Using datamol + RDKit for molecular standardization for preprocessing, toxcast first

smiles_column = "smiles"

def _preprocess(row):
    try:
      mol = dm.to_mol(row[smiles_column], ordered=True)
      mol = dm.fix_mol(mol)
      mol = dm.sanitize_mol(mol, sanifix=True, charge_neutral=False)
      mol = dm.standardize_mol(
          mol,
          disconnect_metals = False,
          normalize = True,
          reionize = True,
          uncharge = False,
          stereo = True,
      )
      row["standard_smiles"] = dm.standardize_smiles(dm.to_smiles(mol))
      row["standard_smiles"] = dm.standardize_smiles(dm.to_smiles(mol))
      row["selfies"] = dm.to_selfies(mol)
      row["selfies"] = dm.to_selfies(mol)
      row["inchi"] = dm.to_inchi(mol)
      row["inchikey"] = dm.to_inchikey(mol)
      return row
    except:
        print("An exception occured.")


toxcast_standardized = toxcast_clean.apply(_preprocess, axis = 1)
print(toxcast_standardized)

# # Using datamol + RDKit for molecular standardization for preprocessing, clintox second

# smiles_column = "smiles"

# def _preprocess(row):
#     try:
#       mol = dm.to_mol(row[smiles_column], ordered=True)
#       mol = dm.fix_mol(mol)
#       mol = dm.sanitize_mol(mol, sanifix=True, charge_neutral=False)
#       mol = dm.standardize_mol(
#           mol,
#           disconnect_metals = False,
#           normalize = True,
#           reionize = True,
#           uncharge = False,
#           stereo = True,
#       )
#       row["standard_smiles"] = dm.standardize_smiles(dm.to_smiles(mol))
#       row["standard_smiles"] = dm.standardize_smiles(dm.to_smiles(mol))
#       row["selfies"] = dm.to_selfies(mol)
#       row["selfies"] = dm.to_selfies(mol)
#       row["inchi"] = dm.to_inchi(mol)
#       row["inchikey"] = dm.to_inchikey(mol)
#       return row
#     except:
#         print("An exception occured.")


# clintox_standardized= clintox_clean.apply(_preprocess, axis = 1)
# print(clintox_standardized)

"""## Merging Datasets"""

#merging datasets together
# merged_sets = pd.concat([clintox_standardized, toxcast_standardized], axis = 1)
# print(merged_sets)
#try different combinations of features for prediction
# maybe since some of them are at different intervals, you could do something where you determine which time interval is best for clinical applications?

"""# TTV Split"""

#remove nans, since molecular standardization added some
# merged_sets_clean = merged_sets.dropna()
# print(merged_sets_clean)
toxcast_standardized_clean = toxcast_standardized.dropna()
print(toxcast_standardized_clean)

# merging selfies columns together
# merged_sets_selfies = merged_sets_clean['selfies']
# merged_sets_selfies = merged_sets_selfies.copy()
# merged_sets_selfies.columns = ['selfies1', 'selfies2']
# merged_sets_selfies['selfies'] = merged_sets_selfies['selfies1'] + merged_sets_selfies['selfies2']
# merged_sets_selfies_1 = merged_sets_selfies.drop(['selfies1', 'selfies2'], axis = 1)

"""#Feature Selection"""

print(toxcast_standardized_clean)

#defining X and y
# X = merged_sets_selfies_1['selfies']
# y = merged_sets_clean['APR_HepG2_CellCycleArrest_24h_dn']

X = toxcast_standardized_clean['selfies']
y = toxcast_standardized_clean['APR_HepG2_CellCycleArrest_24h_dn']

#train-test-validation split, 80/20 train-test split, 75/25 train-validation split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 42)

"""# One-Hot Encoding"""

#encoding training dataframe first

# Example DataFrame with SELFIES column
df_selfies_train = X_train

# Define a function to convert SELFIES string to a one-hot encoding vector
def selfies_to_onehot(selfies_str):
    # List of possible characters in SELFIES
    alphabet = sf.get_alphabet_from_selfies(df_selfies_train)
    alphabet.add("[nop]")
    alphabet = list(sorted(alphabet))

    # Create a one-hot encoded vector
    onehot = np.zeros(len(alphabet), dtype = int)
    for i, char in enumerate(alphabet):
        if char in selfies_str:
            onehot[i] = 1
    return onehot

# Apply the function to convert all SELFIES to one-hot encoding
selfies_vectors = df_selfies_train.apply(selfies_to_onehot)

# Convert the list of one-hot encoded SELFIES to a numpy array
selfies_array_train = np.array(list(selfies_vectors))

# Print the one-hot encoded vectors
print(selfies_array_train)

#encoding validation set

# Example DataFrame with SELFIES column
df_selfies_val = X_val

# Define a function to convert SELFIES string to a one-hot encoding vector
def selfies_to_onehot(selfies_str):
    # List of possible characters in SELFIES
    alphabet = sf.get_alphabet_from_selfies(df_selfies_val)
    alphabet.add("[nop]")
    alphabet = list(sorted(alphabet))

    # Create a one-hot encoded vector
    onehot = np.zeros(len(alphabet), dtype = int)
    for i, char in enumerate(alphabet):
        if char in selfies_str:
            onehot[i] = 1
    return onehot

# Apply the function to convert all SELFIES to one-hot encoding
selfies_vectors = df_selfies_val.apply(selfies_to_onehot)

# Convert the list of one-hot encoded SELFIES to a numpy array
selfies_array_val = np.array(list(selfies_vectors))

# Print the one-hot encoded vectors
print(selfies_array_val)

#encoding test set
# Example DataFrame with SELFIES column
df_selfies_test = X_test

# Define a function to convert SELFIES string to a one-hot encoding vector
def selfies_to_onehot(selfies_str):
    # List of possible characters in SELFIES
    alphabet = sf.get_alphabet_from_selfies(df_selfies_test)
    alphabet.add("[nop]")
    alphabet = list(sorted(alphabet))

    # Create a one-hot encoded vector
    onehot = np.zeros(len(alphabet), dtype = int)
    for i, char in enumerate(alphabet):
        if char in selfies_str:
            onehot[i] = 1
    return onehot

# Apply the function to convert all SELFIES to one-hot encoding
selfies_vectors = df_selfies_test.apply(selfies_to_onehot)

# Convert the list of one-hot encoded SELFIES to a numpy array
selfies_array_test = np.array(list(selfies_vectors))

# Print the one-hot encoded vectors
print(selfies_array_test)

"""# Conversion to Torch Tensors"""

import torch
import torch.nn as nn
import torch.optim as optim

# Convert to torch tensor (training)
X_tox_tensor_train = torch.from_numpy(np.asarray(selfies_array_train))
y_tox_tensor_train = torch.from_numpy(np.asarray(y_train))

#Convert to torch tensor (val)
X_tox_tensor_val = torch.from_numpy(np.asarray(selfies_array_val))
y_tox_tensor_val = torch.from_numpy(np.asarray(y_val))

#Convert to torch tensor (test)
X_tox_tensor_test = torch.from_numpy(np.asarray(selfies_array_test))
y_tox_tensor_test = torch.from_numpy(np.asarray(y_test))

"""#Graph Neural Network (Model Training)"""

#type assignment
X_tox_tensor_train = X_tox_tensor_train.type(torch.FloatTensor)
y_tox_tensor_train = y_tox_tensor_train.type(torch.FloatTensor)

from tensorflow.keras import layers, regularizers, models
import torch.nn.functional as F
model = tf.keras.Sequential([
    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    layers.Dropout(0.5),
    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    layers.Dense(10, activation='softmax')
])

print(X_tox_tensor_train.shape)
print(y_tox_tensor_train.shape)

# inputs/outputs
x_data = X_tox_tensor_train
y_true = y_tox_tensor_train

class Toxicity(nn.Module):
    def __init__(self):
        super(Toxicity, self).__init__()
        self.dropout = nn.Dropout(0.5)
        self.fc1 = nn.Linear(155, 64)
        self.fc2 = nn.Linear(64, 1)

        #add attention layers

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        return self.fc2(x)

# create model, define optimizer and loss function
model = Toxicity()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = nn.MSELoss()

# Training loop (simplified)
for epoch in range(100):  # 100 epochs for training
    optimizer.zero_grad()  # Clear previous gradients

    # Forward pass
    y_pred = model(x_data)  # Get the model's output

    # Compute loss
    loss = criterion(y_pred, y_true) # Compare output with true values

    # Backpropagation (compute gradients)
    loss.backward()

    # Update weights
    optimizer.step()

    # Print loss at each epoch
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')

"""# Model Evaluation"""

#metrics
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve

#detach to numpy
y_true_detach = y_true.detach().numpy()
y_pred_detach = y_pred.detach().numpy()

#convert to binary
y_pred_binary = (y_pred_detach >= 0.5).astype(int)
print(y_pred_binary)

#accuracy
accuracy = accuracy_score(y_true_detach, y_pred_binary, normalize = True)
print(accuracy)

#confusion matrix
confusion_matrix = confusion_matrix(y_true_detach, y_pred_binary)
print(confusion_matrix)

#precision score
precision_score = precision_score(y_true_detach, y_pred_binary, zero_division = 0)
print(precision_score)

#recall score
recall_score = recall_score(y_true_detach, y_pred_binary)
print(recall_score)

#f1 score
f1_score = f1_score(y_true_detach, y_pred_binary)
print(f1_score)

#roc_auc score
roc_auc_score = roc_auc_score(y_true_detach, y_pred_detach)
print(roc_auc_score)

# #roc curve
# fpr, tpr, thresholds = roc_curve(y_true_detach, y_pred_detach)

# output: target variable (non-toxic vs toxic), probability matrix, convert back to non-encoded version of variables,
# probability matrix = probability of toxicity, confusion matrix is for model efficacy
# remainder of time: change learning rates, regression method, add optimizers, change cycles, "distance" measure of differences btwn probability for toxic vs non-toxic
# visualization = ml pipeline to walk thru steps, (quant visual) confusion matrix, testing vs training learning acc curve (how you prevented overfitting)
# diagram for process (!)

"""# Visuals"""