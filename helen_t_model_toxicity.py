# -*- coding: utf-8 -*-
"""Helen T - Model Toxicity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VfIW3p4kZk9TZNhmHCCMLuinzAKiPIBC

#Downloads

Installation for the following:


*   Pytorch Geometric (this might take like 44 minutes to download btw)
*   rdkit
*   selfies
*   datamol
"""

pip install rdkit

pip install selfies

pip install datamol

!pip install --verbose --no-cache-dir torch-scatter
!pip install --verbose --no-cache-dir torch-sparse
!pip install --verbose --no-cache-dir torch-cluster
!pip install torch-geometric
!pip install tensorboardX
!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip ngrok-stable-linux-amd64.zip

"""#Preprocessing"""

from google.colab import drive
drive.mount('/content/drive')

# Load/import libraries
import numpy as np
import pandas as pd
import tensorflow as tf

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve

import matplotlib.pyplot as plt

from rdkit import Chem
from rdkit.Chem import PandasTools
import datamol as dm
import selfies as sf

dm.disable_rdkit_log()

#loading the datasets in

# Idk if this will work on your end because I copied the file path from my Google Drive

tox_21 = '/content/drive/MyDrive/Helen Tran - Science Research 2024-2025/Data/tox21_10k_data_all.sdf'

tox_21 = PandasTools.LoadSDF(tox_21, embedProps = True, molColName = None, smilesName = 'smiles')
clintox = pd.read_csv('/content/drive/MyDrive/Helen Tran - Science Research 2024-2025/Data/clintox.csv')
toxcast = pd.read_csv('/content/drive/MyDrive/Helen Tran - Science Research 2024-2025/Data/toxcast_data.csv')

#NaN = unknown data/no data
#0/1 = no interaction
#remove all rows/columns with NaNs

# tox_df['NR-AR'] just gives the values in a specific column, 'NR-AR' is specific to this dataset
# notna excludes all NaNs
# pd.merge (df1, df2) merges the dfs together
# pd.merge(df1,df2, on=‘Formula’) keeps the Formula column constant, but all the other columns are merged

#merging datasets together
merged_sets = pd.merge(tox_21, clintox, how ='inner', on='smiles').merge(toxcast, how ='inner', on = 'smiles')

#train-test split
train_df, test_df = train_test_split(merged_sets, test_size = 0.2, random_state = 42)

#feature selection

#empty for now

# print(train_df.columns.tolist())

# Using datamol + RDKit for molecular standardization for preprocessing

smiles_column = "smiles"

def _preprocess(row):
    try:
      mol = dm.to_mol(row[smiles_column], ordered=True)
      mol = dm.fix_mol(mol)
      mol = dm.sanitize_mol(mol, sanifix=True, charge_neutral=False)
      mol = dm.standardize_mol(
          mol,
          disconnect_metals = False,
          normalize = True,
          reionize = True,
          uncharge = False,
          stereo = True,
      )
      row["standard_smiles"] = dm.standardize_smiles(dm.to_smiles(mol))
      row["standard_smiles"] = dm.standardize_smiles(dm.to_smiles(mol))
      row["selfies"] = dm.to_selfies(mol)
      row["selfies"] = dm.to_selfies(mol)
      row["inchi"] = dm.to_inchi(mol)
      row["inchikey"] = dm.to_inchikey(mol)
      return row
    except:
        print("An exception occured.")


train_data_clean = train_df.apply(_preprocess, axis = 1)
test_data_clean = test_df.apply(_preprocess, axis = 1)
print(train_data_clean)
print(test_data_clean)

import numpy as np
import pandas as pd

# Example DataFrame with SELFIES column
df_selfies = train_data_clean['selfies']

# Define a function to convert SELFIES string to a one-hot encoding vector
def selfies_to_onehot(selfies_str):
    # List of possible characters in SELFIES
    alphabet = sf.get_alphabet_from_selfies(df_selfies)
    alphabet.add("[nop]")
    alphabet = list(sorted(alphabet))

    # Create a one-hot encoded vector
    onehot = np.zeros(len(alphabet), dtype = int)
    for i, char in enumerate(alphabet):
        if char in selfies_str:
            onehot[i] = 1
    return onehot

# Apply the function to convert all SELFIES to one-hot encoding
selfies_vectors = df_selfies.apply(selfies_to_onehot)

# Convert the list of one-hot encoded SELFIES to a numpy array
selfies_array = np.array(list(selfies_vectors))

# Print the one-hot encoded vectors
print(selfies_array)

import torch
import torch.nn as nn
import torch.optim as optim

# Convert to torch tensor
tox_tensor = torch.from_numpy(selfies_array)

# Print the tensor to check
print(tox_tensor)

"""#Graph Neural Network"""

#inputs/outputs

# x_data =
# y_data =

# a very simple model with just one layer
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(1, 1)  # 1 input, 1 output

    ## add more here if you'd like!

    def forward(self, x):

        ## add more here if you'd like! for now no need
        return self.fc(x)

# create model, define optimizer and loss function
model = SimpleModel()
optimizer = optim.Adam(model.parameters(), lr=0.01)
criterion = nn.MSELoss()

# Training loop (simplified)
for epoch in range(100):  # 100 epochs for training
    optimizer.zero_grad()  # Clear previous gradients

    # Forward pass
    output = model(x_data)  # Get the model's output

    # Compute loss
    loss = criterion(output, y_data)  # Compare output with true values

    # Backpropagation (compute gradients)
    loss.backward()

    # Update weights
    optimizer.step()

    # Print loss at each epoch
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')

#metrics

# sklearn.accuracy_score(y_true, y_pred, normalize = True, sample_weight = None)
# sklearn.metrics.confusion_matrix(y_true, y_pred, labels = None, sample_weight = None, normalize = None)
# sklearn.metrics.precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')
# sklearn.metrics.recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')
# sklearn.metrics.f1_score (y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')
# sklearn.metrics.roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)[source]