# -*- coding: utf-8 -*-
"""Helen T - Model Toxicity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VfIW3p4kZk9TZNhmHCCMLuinzAKiPIBC

#Downloads and Imports

Installation for the following:


*   Pytorch Geometric (this might take like 44 minutes to download btw)
*   rdkit
*   selfies
*   datamol
"""

pip install rdkit

pip install selfies

pip install datamol

!pip install --verbose --no-cache-dir torch-scatter
!pip install --verbose --no-cache-dir torch-sparse
!pip install --verbose --no-cache-dir torch-cluster
!pip install torch-geometric
!pip install tensorboardX
!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip ngrok-stable-linux-amd64.zip

from google.colab import drive
drive.mount('/content/drive')

# Load/import libraries
import numpy as np
import pandas as pd
import tensorflow as tf

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve

import matplotlib.pyplot as plt

from rdkit import Chem
from rdkit.Chem import PandasTools
import datamol as dm
import selfies as sf

dm.disable_rdkit_log()

"""#Preprocessing"""

# loading the datasets in

# Idk if this will work on your end because I copied the file path from my Google Drive

# tox_21 = '/content/drive/MyDrive/Helen Tran - Science Research 2024-2025/Data/tox21_10k_data_all.sdf'

# tox_21 = PandasTools.LoadSDF(tox_21, embedProps = True, molColName = None, smilesName = 'smiles')
#clintox = pd.read_csv('/content/drive/MyDrive/Helen Tran - Science Research 2024-2025/Data/clintox.csv')
toxcast = pd.read_csv('/content/drive/MyDrive/Helen Tran - Science Research 2024-2025/Data/toxcast_data.csv')

#NaN = unknown data/no data
#0/1 = no interaction
#remove all rows/columns with NaNs

# tox_df['NR-AR'] just gives the values in a specific column, 'NR-AR' is specific to this dataset
# notna excludes all NaNs
# pd.merge (df1, df2) merges the dfs together
# pd.merge(df1,df2, on=‘Formula’) keeps the Formula column constant, but all the other columns are merged

"""## Removing Invalid Values"""

# removing nans from toxcast
smiles_data = toxcast['smiles']
feature_data = toxcast.drop(columns = ['smiles'])
feature_data_clean = feature_data.notna().astype(float)
smiles_clean = smiles_data.loc[feature_data_clean.index]
toxcast_clean = feature_data_clean.copy()
toxcast_clean['smiles'] = smiles_clean

toxcast_clean = toxcast_clean.merge(smiles_clean, on = 'smiles')
print(toxcast_clean)

# #removing nans from clintox
# smiles_clintox = clintox['smiles']
# clintox_features= clintox.drop(columns = ['smiles'])
# clintox_features_clean = clintox_features.notna().astype(float)
# smiles_clintox_clean = smiles_clintox.loc[clintox_features_clean.index]
# clintox_clean = clintox_features_clean.copy()
# clintox_clean['smiles'] = smiles_clintox_clean
# print(clintox_clean)

"""## Molecular Standardization"""

# Using datamol + RDKit for molecular standardization for preprocessing, toxcast first

smiles_column = "smiles"

def _preprocess(row):
    try:
      mol = dm.to_mol(row[smiles_column], ordered=True)
      mol = dm.fix_mol(mol)
      mol = dm.sanitize_mol(mol, sanifix=True, charge_neutral=False)
      mol = dm.standardize_mol(
          mol,
          disconnect_metals = False,
          normalize = True,
          reionize = True,
          uncharge = False,
          stereo = True,
      )
      row["standard_smiles"] = dm.standardize_smiles(dm.to_smiles(mol))
      row["standard_smiles"] = dm.standardize_smiles(dm.to_smiles(mol))
      row["selfies"] = dm.to_selfies(mol)
      row["selfies"] = dm.to_selfies(mol)
      row["inchi"] = dm.to_inchi(mol)
      row["inchikey"] = dm.to_inchikey(mol)
      return row
    except:
        print("An exception occured.")


toxcast_standardized = toxcast_clean.apply(_preprocess, axis = 1)
print(toxcast_standardized)

# # Using datamol + RDKit for molecular standardization for preprocessing, clintox second

# smiles_column = "smiles"

# def _preprocess(row):
#     try:
#       mol = dm.to_mol(row[smiles_column], ordered=True)
#       mol = dm.fix_mol(mol)
#       mol = dm.sanitize_mol(mol, sanifix=True, charge_neutral=False)
#       mol = dm.standardize_mol(
#           mol,
#           disconnect_metals = False,
#           normalize = True,
#           reionize = True,
#           uncharge = False,
#           stereo = True,
#       )
#       row["standard_smiles"] = dm.standardize_smiles(dm.to_smiles(mol))
#       row["standard_smiles"] = dm.standardize_smiles(dm.to_smiles(mol))
#       row["selfies"] = dm.to_selfies(mol)
#       row["selfies"] = dm.to_selfies(mol)
#       row["inchi"] = dm.to_inchi(mol)
#       row["inchikey"] = dm.to_inchikey(mol)
#       return row
#     except:
#         print("An exception occured.")


# clintox_standardized= clintox_clean.apply(_preprocess, axis = 1)
# print(clintox_standardized)

"""## Merging Datasets"""

#merging datasets together
# merged_sets = pd.concat([clintox_standardized, toxcast_standardized], axis = 1)
# print(merged_sets)
#try different combinations of features for prediction
# maybe since some of them are at different intervals, you could do something where you determine which time interval is best for clinical applications?

"""# TTV Split"""

#remove nans, since molecular standardization added some
# merged_sets_clean = merged_sets.dropna()
# print(merged_sets_clean)
toxcast_standardized_clean = toxcast_standardized.dropna()
print(toxcast_standardized_clean)

# merging selfies columns together
# merged_sets_selfies = merged_sets_clean['selfies']
# merged_sets_selfies = merged_sets_selfies.copy()
# merged_sets_selfies.columns = ['selfies1', 'selfies2']
# merged_sets_selfies['selfies'] = merged_sets_selfies['selfies1'] + merged_sets_selfies['selfies2']
# merged_sets_selfies_1 = merged_sets_selfies.drop(['selfies1', 'selfies2'], axis = 1)

#defining X and y
# X = merged_sets_selfies_1['selfies']
# y = merged_sets_clean['APR_HepG2_CellCycleArrest_24h_dn']

X = toxcast_standardized_clean['selfies']
y = toxcast_standardized_clean['APR_HepG2_CellCycleArrest_24h_dn']

#train-test-validation split, 80/20 train-test split, 75/25 train-validation split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 42)

"""# One-Hot Encoding"""

#encoding training dataframe first

# Example DataFrame with SELFIES column
df_selfies_train = X_train

# Define a function to convert SELFIES string to a one-hot encoding vector
def selfies_to_onehot(selfies_str):
    # List of possible characters in SELFIES
    alphabet = sf.get_alphabet_from_selfies(df_selfies_train)
    alphabet.add("[nop]")
    alphabet = list(sorted(alphabet))

    # Create a one-hot encoded vector
    onehot = np.zeros(len(alphabet), dtype = int)
    for i, char in enumerate(alphabet):
        if char in selfies_str:
            onehot[i] = 1
    return onehot

# Apply the function to convert all SELFIES to one-hot encoding
selfies_vectors = df_selfies_train.apply(selfies_to_onehot)

# Convert the list of one-hot encoded SELFIES to a numpy array
selfies_array_train = np.array(list(selfies_vectors))

# Print the one-hot encoded vectors
print(selfies_array_train)

#encoding validation set

# Example DataFrame with SELFIES column
df_selfies_val = X_val

# Define a function to convert SELFIES string to a one-hot encoding vector
def selfies_to_onehot(selfies_str):
    # List of possible characters in SELFIES
    alphabet = sf.get_alphabet_from_selfies(df_selfies_val)
    alphabet.add("[nop]")
    alphabet = list(sorted(alphabet))

    # Create a one-hot encoded vector
    onehot = np.zeros(len(alphabet), dtype = int)
    for i, char in enumerate(alphabet):
        if char in selfies_str:
            onehot[i] = 1
    return onehot

# Apply the function to convert all SELFIES to one-hot encoding
selfies_vectors = df_selfies_val.apply(selfies_to_onehot)

# Convert the list of one-hot encoded SELFIES to a numpy array
selfies_array_val = np.array(list(selfies_vectors))

# Print the one-hot encoded vectors
print(selfies_array_val)

#encoding test set
# Example DataFrame with SELFIES column
df_selfies_test = X_test

# Define a function to convert SELFIES string to a one-hot encoding vector
def selfies_to_onehot(selfies_str):
    # List of possible characters in SELFIES
    alphabet = sf.get_alphabet_from_selfies(df_selfies_test)
    alphabet.add("[nop]")
    alphabet = list(sorted(alphabet))

    # Create a one-hot encoded vector
    onehot = np.zeros(len(alphabet), dtype = int)
    for i, char in enumerate(alphabet):
        if char in selfies_str:
            onehot[i] = 1
    return onehot

# Apply the function to convert all SELFIES to one-hot encoding
selfies_vectors = df_selfies_test.apply(selfies_to_onehot)

# Convert the list of one-hot encoded SELFIES to a numpy array
selfies_array_test = np.array(list(selfies_vectors))

# Print the one-hot encoded vectors
print(selfies_array_test)

"""# Conversion to Torch Tensors"""

import torch
import torch.nn as nn
import torch.optim as optim

# Convert to torch tensor (training)
X_tox_tensor_train = torch.from_numpy(np.asarray(selfies_array_train))
y_tox_tensor_train = torch.from_numpy(np.asarray(y_train))

#Convert to torch tensor (val)
X_tox_tensor_val = torch.from_numpy(np.asarray(selfies_array_val))
y_tox_tensor_val = torch.from_numpy(np.asarray(y_val))

#Convert to torch tensor (test)
X_tox_tensor_test = torch.from_numpy(np.asarray(selfies_array_test))
y_tox_tensor_test = torch.from_numpy(np.asarray(y_test))

"""#Graph Neural Network (Model Training)"""

#type assignment

X_tox_tensor_train = X_tox_tensor_train.type(torch.FloatTensor)
y_tox_tensor_train = y_tox_tensor_train.type(torch.FloatTensor)

# inputs/outputs
# try more things (you can talk abt this too), add diff layers (linear layers, pre-existing ones)

x_data = X_tox_tensor_train
y_data = y_tox_tensor_train

# a very simple model with just one layer
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(155, 1)  # 1 input, 1 output

    ## add more here if you'd like!

    def forward(self, x):

        ## add more here if you'd like! for now no need
        return self.fc(x)

# create model, define optimizer and loss function
model = SimpleModel()
optimizer = optim.Adam(model.parameters(), lr=0.01)
criterion = nn.MSELoss()

# Training loop (simplified)
for epoch in range(100):  # 100 epochs for training
    optimizer.zero_grad()  # Clear previous gradients

    # Forward pass
    output = model(x_data)  # Get the model's output

    # Compute loss
    loss = criterion(output, y_data)  # Compare output with true values

    # Backpropagation (compute gradients)
    loss.backward()

    # Update weights
    optimizer.step()

    # Print loss at each epoch
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')

"""# Model Evaluation"""

#metrics

# sklearn.accuracy_score(y_true, y_pred, normalize = True, sample_weight = None)
# sklearn.metrics.confusion_matrix(y_true, y_pred, labels = None, sample_weight = None, normalize = None)
# sklearn.metrics.precision_score(y_true, y_pred, *, labels = None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')
# sklearn.metrics.recall_score(y_true, y_pred, *, label = None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')
# sklearn.metrics.f1_score (y_true, y_pred, *, labels = None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')
# sklearn.metrics.roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)[source]

# output: target variable (non-toxic vs toxic), probability matrix, convert back to non-encoded version of variables,
# probability matrix = probability of toxicity, confusion matrix is for model efficacy
# remainder of time: change learning rates, regression method, add optimizers, change cycles, "distance" measure of differences btwn probability for toxic vs non-toxic
# visualization = ml pipeline to walk thru steps, (quant visual) confusion matrix, testing vs training learning acc curve (how you prevented overfitting)
# diagram for process (!)